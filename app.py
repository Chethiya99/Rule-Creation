import streamlit as st
import pandas as pd
import groq
import json
from typing import List, Dict, Any
import os
import time

# Initialize Groq client
client = groq.Client(api_key=st.secrets["GROQ_API_KEY"])

# Define CSV file structures (this would be auto-generated by analyzing the actual files)
CSV_STRUCTURES = {
    "sample_mortgage_accounts.csv": ["customer_id", "account_status", "loan_amount", "interest_rate", "start_date", "term"],
    "sample_loan_repayments.csv": ["transaction_id", "customer_id", "payment_amount", "payment_date", "loan_id", "status"],
    "sample_telco_billing.csv": ["bill_id", "customer_id", "amount", "due_date", "paid_date", "service_type"],
    "sample_product_enrollments.csv": ["enrollment_id", "customer_id", "product_id", "enrollment_date", "status"],
    "sample_customer_profiles.csv": ["customer_id", "name", "age", "income", "credit_score", "address"],
    "sample_savings_account_transactions.csv": ["transaction_id", "account_id", "customer_id", "amount", "date", "transaction_type"],
    "sample_credit_card_transactions.csv": ["transaction_id", "card_id", "customer_id", "amount", "date", "merchant", "category"]
}

# Sample data for demonstration (in production, we'd read actual CSVs)
SAMPLE_DATA = {
    "sample_mortgage_accounts.csv": pd.DataFrame({
        "customer_id": ["C001", "C002", "C003"],
        "account_status": ["active", "closed", "active"],
        "loan_amount": [250000, 180000, 300000],
        "interest_rate": [3.5, 4.2, 3.8],
        "start_date": ["2020-01-15", "2019-05-20", "2021-03-10"],
        "term": [30, 25, 30]
    }),
    "sample_credit_card_transactions.csv": pd.DataFrame({
        "transaction_id": ["T001", "T002", "T003", "T004"],
        "card_id": ["CRD001", "CRD001", "CRD002", "CRD003"],
        "customer_id": ["C001", "C001", "C002", "C003"],
        "amount": [1200, 1500, 800, 2600],
        "date": ["2023-10-15", "2023-10-20", "2023-10-18", "2023-10-25"],
        "merchant": ["Amazon", "Best Buy", "Walmart", "Apple"],
        "category": ["Shopping", "Electronics", "Groceries", "Electronics"]
    })
}

def analyze_csv_structure(file_path: str) -> List[str]:
    """Analyze a CSV file and return its columns"""
    try:
        df = pd.read_csv(file_path)
        return list(df.columns)
    except Exception as e:
        st.error(f"Error reading {file_path}: {str(e)}")
        return []

def generate_prompt_guidance(user_input: str) -> str:
    """Generate guidance for the AI based on user input and available data"""
    available_data = "\n".join([f"- {f}: {', '.join(cols)}" for f, cols in CSV_STRUCTURES.items()])
    
    return f"""
    You are a financial rule generation assistant. Your task is to help create rules for mortgage holders based on available data sources.

    Available data sources and their columns:
    {available_data}

    The user has provided this requirement: "{user_input}"

    Analyze this requirement and:
    1. Identify which data sources are needed
    2. Determine which columns from each source should be used
    3. Create a logical rule structure with proper AND/OR conditions
    4. Output the rule in JSON format matching this schema:
        {{
            "rules": [
                {{
                    "id": "generated_id",
                    "dataSource": "source_name",
                    "field": "column_name",
                    "eligibilityPeriod": "time_period or N/A",
                    "function": "aggregation_function or N/A",
                    "operator": "comparison_operator",
                    "value": "comparison_value",
                    "priority": null,
                    "ruleType": "condition" or "conditionGroup",
                    "connector": "AND" or "OR" or null,
                    "conditions": [ /* for conditionGroup only */ ]
                }}
            ]
        }}

    Respond ONLY with the JSON output. Do not include any additional explanation or markdown formatting.
    The rule should be as specific as possible to match the user's requirements.
    """

def generate_rule_with_llama(user_input: str) -> Dict[str, Any]:
    """Use Groq/Llama to generate a rule based on user input"""
    prompt = generate_prompt_guidance(user_input)
    
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are a financial rule generation expert that creates precise JSON rules based on data sources."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model="llama3-70b-8192",
            temperature=0.3
        )
        
        response_content = chat_completion.choices[0].message.content
        
        # Clean the response to extract just the JSON
        json_str = response_content[response_content.find('{'):response_content.rfind('}')+1]
        return json.loads(json_str)
    
    except Exception as e:
        st.error(f"Error generating rule: {str(e)}")
        return None

def display_rule_ui(rule: Dict[str, Any], group_index: int = 0, condition_index: int = 0) -> None:
    """Display the rule in the UI similar to the provided images"""
    if not rule:
        st.warning("No rule generated yet")
        return
    
    st.subheader("Rule Conditions")
    st.markdown("Define the logical conditions for this rule to apply.")
    
    # Priority checkbox
    st.checkbox("Enable priority order and drag & drop", key=f"priority_{group_index}")
    
    # Main rule display
    for i, rule_item in enumerate(rule.get("rules", [])):
        if rule_item["ruleType"] == "condition":
            with st.expander(f"Condition {i+1}", expanded=True):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.selectbox("Data Source", [rule_item["dataSource"]], key=f"ds_{i}_{group_index}")
                with col2:
                    st.selectbox("Field", [rule_item["field"]], key=f"field_{i}_{group_index}")
                with col3:
                    st.selectbox("Operator", [rule_item["operator"]], key=f"op_{i}_{group_index}")
                with col4:
                    st.text_input("Value", rule_item["value"], key=f"val_{i}_{group_index}")
                
                if i < len(rule["rules"]) - 1:
                    st.selectbox("Connector", [rule_item.get("connector", "AND")], key=f"conn_{i}_{group_index}")
        
        elif rule_item["ruleType"] == "conditionGroup":
            with st.expander(f"Condition Group {i+1}", expanded=True):
                st.markdown("#### Condition Group")
                for j, condition in enumerate(rule_item.get("conditions", [])):
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.selectbox("Data Source", [condition["dataSource"]], key=f"gds_{i}_{j}_{group_index}")
                    with col2:
                        st.selectbox("Field", [condition["field"]], key=f"gfield_{i}_{j}_{group_index}")
                    with col3:
                        st.selectbox("Operator", [condition["operator"]], key=f"gop_{i}_{j}_{group_index}")
                    with col4:
                        st.text_input("Value", condition["value"], key=f"gval_{i}_{j}_{group_index}")
                    
                    if j < len(rule_item["conditions"]) - 1:
                        st.selectbox("Connector", [condition.get("connector", "AND")], key=f"gconn_{i}_{j}_{group_index}")

def chat_interface() -> str:
    """Render the chat interface and return the user's final prompt"""
    st.sidebar.title("Rule Assistant (Llama 3)")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": "Hello! I can help you create mortgage holder rules using Llama 3. What criteria would you like to use?"})
    
    for message in st.session_state.messages:
        with st.sidebar.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if prompt := st.sidebar.chat_input("What rule would you like to create?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.sidebar.chat_message("user"):
            st.markdown(prompt)
        
        # Generate initial response
        with st.sidebar.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Generate rule and get confirmation question
            with st.spinner("Analyzing with Llama 3..."):
                rule = generate_rule_with_llama(prompt)
                if rule:
                    st.session_state.current_rule = rule
                    assistant_response = f"I've generated a rule for:\n\n{prompt}\n\nHere's what I created:\n\n```json\n{json.dumps(rule, indent=2)}\n```\n\nDoes this look correct?"
                else:
                    assistant_response = "I couldn't generate a rule for that request. Could you please provide more details?"
            
            # Simulate streaming response
            for chunk in assistant_response.split():
                full_response += chunk + " "
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.05)
            
            message_placeholder.markdown(full_response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        return prompt
    
    return ""

def main():
    st.set_page_config(page_title="Mortgage Rule Generator (Llama 3)", layout="wide")
    st.title("Mortgage Rule Generator with Llama 3")
    
    # Initialize session state
    if "current_rule" not in st.session_state:
        st.session_state.current_rule = None
    if "confirmed" not in st.session_state:
        st.session_state.confirmed = False
    
    # Create two columns - main UI and chat
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # Get user input through chat
        user_prompt = chat_interface()
        
        if user_prompt and not st.session_state.confirmed:
            # Display the generated rule
            if st.session_state.current_rule:
                display_rule_ui(st.session_state.current_rule)
                
                # Confirmation buttons
                st.write("Is this rule correct?")
                confirm_col, modify_col = st.columns(2)
                
                with confirm_col:
                    if st.button("Yes, this is correct"):
                        st.session_state.confirmed = True
                        st.success("Rule confirmed! You can now export the JSON.")
                        st.json(st.session_state.current_rule)
                
                with modify_col:
                    if st.button("No, I need changes"):
                        st.session_state.messages.append({"role": "user", "content": "Please modify the rule based on these changes..."})
                        st.info("Please describe the changes needed in the chat.")
            else:
                st.warning("No rule was generated. Please try again with more details.")
        
        elif st.session_state.confirmed:
            # Show confirmed rule
            st.success("Confirmed Rule")
            display_rule_ui(st.session_state.current_rule)
            st.json(st.session_state.current_rule)
            
            if st.button("Create New Rule"):
                st.session_state.confirmed = False
                st.session_state.current_rule = None
                st.session_state.messages = []
                st.experimental_rerun()
    
    with col2:
        st.markdown("""
        <style>
            .stChatFloatingInputContainer {
                bottom: 20px;
            }
            .stChatMessage {
                padding: 12px;
                border-radius: 8px;
                margin-bottom: 12px;
            }
            .assistant-message {
                background-color: #f0f2f6;
            }
            .user-message {
                background-color: #e3f2fd;
            }
        </style>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
